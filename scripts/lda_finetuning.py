# -*- coding: utf-8 -*-
"""LDA_FineTuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GjaK_dKnhN2Om35ruxpDzwcuPwaUjoUR
"""

!pip install -q pyLDAvis

import matplotlib.pyplot as plt
import numpy as np
import os
import operator
from nltk.corpus import stopwords
import gensim
from gensim import corpora, models
# from textblob import TextBlob
from bs4 import BeautifulSoup
import pyLDAvis.gensim_models 
import pickle 
import pyLDAvis
import tqdm

import nltk
from nltk.stem import WordNetLemmatizer
  
lemmatizer = WordNetLemmatizer()
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')

from nltk.corpus import stopwords
import spacy
from spacy import displacy
from collections import Counter
import en_core_web_sm
nlp = en_core_web_sm.load()
import spacy
#loading the english language small model of spacy
en = spacy.load('en_core_web_sm')
sw_spacy = en.Defaults.stop_words
print(sw_spacy)

def preprocess(wordlist):
  text=' '.join(wordlist)
  tokens = nltk.word_tokenize(text)
  l_nouns = [lemmatizer.lemmatize(word) for word in tokens]
  tags = nltk.tag.pos_tag(l_nouns)
  nouns = [word.lower() for word,pos in tags if (pos == 'NN' or pos == 'NNS')] 
  return nouns

preprocess(["Today","Indian","student's","joel","Thanks","Regards","IIT","Palakkad","Anoop","George","session","research","forums","forum"])

from google.colab import drive
drive.mount('/content/drive')

"""#Reading Data"""

doc = {}
doc_words = [[]]
termdf = {}
msg_doc = {}
no_of_docs = 0
msg_line_count = 0
filename = "/content/drive/MyDrive/BTP_Aishu/mail_bodies.txt"

for line in open(filename, encoding="utf-8"):
        line = line.strip()
        if(line == "*****************************************************"):
            #doc_words[-1] = preprocess(doc_words[-1])
            doc_words.append([])
            msg_line_count = 0
            no_of_docs += 1
            for term in msg_doc:
                if (doc.__contains__(term)):
                    doc[term] = int(doc.get(term)) + msg_doc[term]
                else:
                    doc[term] = 1
            msg_doc.clear()
        else:
            msg_line_count+=1
            split = line.split(' ')
            for entry in split:
                # entry = entry.lower()
                if(entry in stopwords.words('english') or entry in sw_spacy or (not entry.isalnum()) or len(entry)<=2):
                    continue
                doc_words[-1].append(entry)
                if (msg_doc.__contains__(entry)):
                    msg_doc[entry] = int(msg_doc.get(entry)) + 1
                else:
                    msg_doc[entry] = 1
                    if(termdf.__contains__(entry)):
                        termdf[entry] = int(termdf.get(entry)) + 1
                    else:
                        termdf[entry] = 1

for i in range(len(doc_words)):
  doc_words[i] = preprocess(doc_words[i])

doc_words

tfidf = {}
for term in doc:
    tfidf[term] = doc[term] * np.log(no_of_docs/termdf[term])

print("num docs", no_of_docs)

dictionary = corpora.Dictionary(doc_words)
corpus = [dictionary.doc2bow(text) for text in doc_words]

"""# LDA Model 

Only need to run if you want a new model
"""

num_topics = 11
passes = 20
lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics = num_topics, id2word = dictionary, passes = passes)

LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)
pyLDAvis.display(LDAvis_prepared)

"""# Save and Load the Model"""

from gensim import  models
from gensim.test.utils import datapath

temp_file = datapath("/content/drive/MyDrive/lda_model_11_dummy")

"""### Save"""

lda_model.save(temp_file)

"""### Load

"""

lda_model = models.ldamodel.LdaModel.load(temp_file)

"""# Topic dictionary"""

tops = lda_model.print_topics(num_words = 50)
tops

# tops = [(0,
#   '0.034*"palakkad" + 0.030*"placement" + 0.028*"institute" + 0.021*"kumar" + 0.020*"iit" + 0.018*"contact" + 0.017*"student" + 0.016*"technology" + 0.014*"internship" + 0.014*"training" + 0.013*"representative" + 0.011*"application" + 0.011*"officer" + 0.010*"deadline" + 0.010*"post" + 0.010*"cdc" + 0.009*"batch" + 0.009*"year" + 0.009*"candidate" + 0.009*"information" + 0.008*"coordinator" + 0.008*"test" + 0.007*"link" + 0.007*"interview" + 0.007*"sep" + 0.007*"scholarship" + 0.007*"indian" + 0.007*"career" + 0.007*"oct" + 0.007*"affairs" + 0.007*"process" + 0.006*"profile" + 0.006*"yagnesh" + 0.006*"job" + 0.006*"registration" + 0.006*"aug" + 0.005*"form" + 0.005*"development" + 0.005*"peer" + 0.005*"detail" + 0.005*"sac" + 0.005*"feel" + 0.005*"email" + 0.005*"mail" + 0.005*"thank" + 0.004*"election" + 0.004*"august" + 0.004*"head" + 0.004*"katragadda" + 0.004*"dear"'),
#  (1,
#   '0.024*"student" + 0.019*"innovation" + 0.018*"iit" + 0.013*"team" + 0.013*"message" + 0.012*"information" + 0.010*"india" + 0.009*"program" + 0.008*"palakkad" + 0.008*"technology" + 0.008*"challenge" + 0.008*"project" + 0.008*"opportunity" + 0.007*"nikhil" + 0.007*"year" + 0.007*"council" + 0.006*"session" + 0.006*"detail" + 0.006*"details" + 0.006*"idea" + 0.006*"event" + 0.005*"link" + 0.005*"industry" + 0.005*"problem" + 0.005*"help" + 0.005*"share" + 0.004*"college" + 0.004*"email" + 0.004*"business" + 0.004*"registration" + 0.004*"students" + 0.004*"application" + 0.004*"prize" + 0.004*"onbehalf" + 0.004*"support" + 0.004*"solution" + 0.003*"case" + 0.003*"behalf" + 0.003*"platform" + 0.003*"skill" + 0.003*"experience" + 0.003*"development" + 0.003*"product" + 0.003*"research" + 0.003*"work" + 0.003*"dear" + 0.003*"education" + 0.003*"speaker" + 0.003*"world" + 0.003*"school"'),
#  (2,
#   '0.024*"team" + 0.019*"sports" + 0.019*"singh" + 0.018*"event" + 0.016*"google" + 0.013*"time" + 0.012*"competition" + 0.011*"form" + 0.009*"petrichor" + 0.009*"email" + 0.009*"iit" + 0.009*"response" + 0.009*"invitation" + 0.007*"batch" + 0.007*"affairs" + 0.007*"video" + 0.007*"game" + 0.007*"account" + 0.006*"rituraj" + 0.006*"people" + 0.006*"jul" + 0.006*"yoga" + 0.006*"music" + 0.006*"list" + 0.006*"pm" + 0.005*"zoom" + 0.005*"hope" + 0.005*"thanks" + 0.005*"change" + 0.005*"activity" + 0.005*"hrishi" + 0.005*"meet" + 0.005*"raaj" + 0.005*"member" + 0.005*"guest" + 0.005*"tutorial" + 0.005*"fitness" + 0.005*"link" + 0.005*"greetings" + 0.004*"college" + 0.004*"day" + 0.004*"dec" + 0.004*"organizer" + 0.004*"secretary" + 0.004*"club" + 0.004*"join" + 0.004*"manoranjan" + 0.004*"player" + 0.004*"fill" + 0.004*"room"'),
#  (3,
#   '0.037*"institute" + 0.029*"indian" + 0.027*"technology" + 0.026*"engineering" + 0.025*"palakkad" + 0.024*"iit" + 0.022*"professor" + 0.020*"research" + 0.019*"assistant" + 0.017*"science" + 0.012*"department" + 0.011*"associate" + 0.011*"talk" + 0.010*"computer" + 0.010*"kerala" + 0.009*"india" + 0.007*"ahalia" + 0.006*"faculty" + 0.006*"university" + 0.006*"data" + 0.005*"systems" + 0.005*"sunil" + 0.005*"vinod" + 0.005*"national" + 0.005*"ieee" + 0.005*"conference" + 0.005*"dean" + 0.005*"thanks" + 0.005*"mechanical" + 0.005*"director" + 0.005*"kumar" + 0.005*"chemistry" + 0.004*"uma" + 0.004*"lecture" + 0.004*"paper" + 0.004*"centre" + 0.004*"detail" + 0.004*"meeting" + 0.004*"transit" + 0.004*"academy" + 0.004*"machine" + 0.003*"series" + 0.003*"room" + 0.003*"prasad" + 0.003*"phd" + 0.003*"kozhippara" + 0.003*"patient" + 0.003*"colloquium" + 0.003*"sushabhan" + 0.003*"award"'),
#  (4,
#   '0.028*"answer" + 0.014*"min" + 0.011*"people" + 0.010*"time" + 0.010*"thing" + 0.010*"type" + 0.009*"control" + 0.009*"read" + 0.008*"year" + 0.008*"word" + 0.007*"day" + 0.007*"campus" + 0.007*"language" + 0.007*"life" + 0.007*"world" + 0.006*"way" + 0.006*"email" + 0.006*"body" + 0.005*"sandhya" + 0.005*"friend" + 0.005*"work" + 0.005*"person" + 0.004*"stories" + 0.004*"point" + 0.004*"movie" + 0.004*"use" + 0.004*"money" + 0.004*"support" + 0.003*"mind" + 0.003*"data" + 0.003*"english" + 0.003*"jee" + 0.003*"love" + 0.003*"question" + 0.003*"chandran" + 0.003*"business" + 0.003*"sanjay" + 0.003*"sentence" + 0.003*"article" + 0.003*"help" + 0.003*"fact" + 0.003*"services" + 0.003*"place" + 0.003*"verb" + 0.002*"experience" + 0.002*"lot" + 0.002*"practice" + 0.002*"job" + 0.002*"woman" + 0.002*"end"'),
#  (5,
#   '0.029*"class" + 0.022*"forum" + 0.020*"change" + 0.019*"lab" + 0.018*"assignment" + 0.017*"digest" + 0.016*"forums" + 0.016*"course" + 0.015*"question" + 0.014*"exam" + 0.013*"quiz" + 0.013*"lecture" + 0.013*"dear" + 0.012*"time" + 0.010*"announcements" + 0.009*"week" + 0.009*"submission" + 0.009*"video" + 0.009*"student" + 0.008*"note" + 0.008*"link" + 0.008*"problem" + 0.008*"book" + 0.007*"meeting" + 0.007*"mark" + 0.007*"today" + 0.006*"zoom" + 0.006*"evaluation" + 0.005*"tomorrow" + 0.005*"november" + 0.005*"feedback" + 0.005*"answer" + 0.005*"number" + 0.005*"end" + 0.005*"test" + 0.005*"semester" + 0.005*"batch" + 0.004*"discussion" + 0.004*"hope" + 0.004*"nov" + 0.004*"october" + 0.004*"code" + 0.004*"sheet" + 0.004*"solution" + 0.004*"library" + 0.004*"message" + 0.004*"unsubscribe" + 0.004*"grade" + 0.004*"moodle" + 0.004*"file"'),
#  (6,
#   '0.027*"research" + 0.013*"meeting" + 0.011*"seminar" + 0.010*"system" + 0.009*"zoom" + 0.009*"proposal" + 0.009*"analysis" + 0.009*"detail" + 0.008*"work" + 0.008*"design" + 0.008*"study" + 0.007*"material" + 0.006*"model" + 0.006*"method" + 0.006*"iit" + 0.006*"talk" + 0.006*"performance" + 0.005*"view" + 0.005*"office" + 0.005*"data" + 0.005*"time" + 0.005*"quantum" + 0.005*"palakkad" + 0.005*"technique" + 0.005*"link" + 0.005*"power" + 0.005*"email" + 0.004*"vehicle" + 0.004*"graph" + 0.004*"application" + 0.004*"structure" + 0.004*"control" + 0.004*"number" + 0.004*"algorithm" + 0.004*"date" + 0.004*"space" + 0.004*"dean" + 0.004*"problem" + 0.004*"network" + 0.004*"process" + 0.004*"systems" + 0.004*"presentation" + 0.004*"stress" + 0.003*"reminder" + 0.003*"contact" + 0.003*"adobe" + 0.003*"energy" + 0.003*"detection" + 0.003*"effect" + 0.003*"hpc"'),
#  (7,
#   '0.062*"session" + 0.032*"iit" + 0.032*"link" + 0.022*"club" + 0.019*"palakkad" + 0.013*"form" + 0.012*"workshop" + 0.012*"contact" + 0.011*"thanks" + 0.010*"event" + 0.009*"google" + 0.009*"detail" + 0.008*"competition" + 0.008*"follow" + 0.008*"meeting" + 0.007*"alumni" + 0.007*"student" + 0.006*"affairs" + 0.006*"hope" + 0.005*"registration" + 0.005*"technical" + 0.005*"secretary" + 0.005*"arts" + 0.005*"logo" + 0.005*"deadline" + 0.005*"mohit" + 0.005*"entry" + 0.005*"programming" + 0.005*"team" + 0.005*"paper" + 0.005*"cell" + 0.005*"webinar" + 0.004*"head" + 0.004*"point" + 0.004*"mail" + 0.004*"contest" + 0.004*"member" + 0.004*"quiz" + 0.004*"talk" + 0.004*"participation" + 0.004*"opportunity" + 0.004*"reminder" + 0.004*"training" + 0.004*"time" + 0.004*"robotics" + 0.004*"introduction" + 0.004*"design" + 0.004*"fill" + 0.003*"san" + 0.003*"september"'),
#  (8,
#   '0.042*"student" + 0.030*"institute" + 0.029*"palakkad" + 0.025*"campus" + 0.023*"affairs" + 0.019*"ahalia" + 0.018*"indian" + 0.018*"hostel" + 0.016*"technology" + 0.016*"secretary" + 0.014*"form" + 0.013*"iit" + 0.013*"nila" + 0.012*"thanks" + 0.009*"day" + 0.009*"council" + 0.007*"students" + 0.007*"dean" + 0.007*"general" + 0.006*"registrar" + 0.006*"time" + 0.006*"staff" + 0.006*"semester" + 0.006*"mess" + 0.006*"fill" + 0.006*"faculty" + 0.006*"course" + 0.006*"fee" + 0.005*"contact" + 0.005*"number" + 0.005*"registration" + 0.005*"academics" + 0.005*"yaseen" + 0.005*"request" + 0.005*"room" + 0.004*"case" + 0.004*"date" + 0.004*"office" + 0.004*"mail" + 0.004*"person" + 0.004*"service" + 0.004*"bus" + 0.004*"dear" + 0.004*"google" + 0.004*"academic" + 0.004*"payment" + 0.004*"kozhippara" + 0.004*"year" + 0.003*"batch" + 0.003*"aug"'),
#  (9,
#   '0.030*"internship" + 0.028*"biju" + 0.025*"campus" + 0.023*"inr" + 0.019*"joel" + 0.019*"kerala" + 0.019*"ahalia" + 0.017*"dist" + 0.017*"kozhippara" + 0.017*"development" + 0.016*"sam" + 0.015*"thanks" + 0.013*"mathew" + 0.013*"vivek" + 0.013*"officer" + 0.012*"palakkad" + 0.012*"iit" + 0.012*"jan" + 0.010*"start" + 0.010*"technical" + 0.010*"computer" + 0.010*"mar" + 0.009*"work" + 0.009*"chaturvedi" + 0.008*"service" + 0.008*"matlab" + 0.007*"apr" + 0.007*"attendance" + 0.007*"internet" + 0.006*"gaming" + 0.006*"inconvenience" + 0.006*"software" + 0.006*"jun" + 0.006*"team" + 0.006*"email" + 0.006*"john" + 0.005*"company" + 0.005*"situation" + 0.005*"lab" + 0.005*"solutions" + 0.005*"connectivity" + 0.004*"jerry" + 0.004*"feb" + 0.004*"list" + 0.004*"thomas" + 0.004*"dear" + 0.004*"design" + 0.004*"download" + 0.004*"nov" + 0.004*"app"'),
#  (10,
#   '0.051*"course" + 0.032*"anoop" + 0.023*"email" + 0.021*"george" + 0.018*"coursera" + 0.018*"university" + 0.016*"iit" + 0.014*"google" + 0.013*"social" + 0.013*"sciences" + 0.013*"department" + 0.012*"palakkad" + 0.012*"humanities" + 0.012*"nptel" + 0.012*"kerala" + 0.012*"unsubscribe" + 0.011*"view" + 0.011*"group" + 0.010*"learning" + 0.009*"web" + 0.009*"summary" + 0.009*"specialization" + 0.009*"machine" + 0.008*"oct" + 0.008*"data" + 0.008*"dec" + 0.008*"visit" + 0.007*"groups" + 0.006*"regard" + 0.006*"discussion" + 0.006*"topic" + 0.006*"message" + 0.005*"science" + 0.005*"trademark" + 0.005*"class" + 0.005*"lecture" + 0.005*"introduction" + 0.005*"certificate" + 0.005*"aug" + 0.005*"python" + 0.005*"learner" + 0.005*"ibm" + 0.004*"dear" + 0.004*"ethics" + 0.004*"thanks" + 0.004*"today" + 0.004*"enroll" + 0.004*"help" + 0.004*"language" + 0.004*"time"')]

def parse_topic_model_input(input_list):
    output_list = {}
    all_words = {}
    for item in input_list:
        index, word_weights = item
        output_list[index] = {}
        words = [word.split("*")[1].strip().strip('"') for word in word_weights.split(" + ")]
        probs = [word.split("*")[0].strip().strip('"') for word in word_weights.split(" + ")]
        for i in range(len(words)):
          output_list[index][words[i]] = float(probs[i])
          if words[i] not in all_words:
            all_words[words[i]] = 1
          else: 
            all_words[words[i]] += 1
    return output_list,all_words

Tags_new,all_words = parse_topic_model_input(tops)

Tags_new

def probability_normalizer():
  for topic in Tags_new:
    for word in Tags_new[topic]:
      if all_words[word] > 1:
        #print(Tags_new[topic][word])
        Tags_new[topic][word] = Tags_new[topic][word]/all_words[word]
    Tags_new[topic] = dict(sorted(Tags_new[topic].items(), key=lambda x:x[1],reverse=True))

probability_normalizer()

Tags_new

"""# Testing

No need to run
"""

nltk.download('stopwords')
nltk.download('punkt')
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.util import ngrams

def extract_keywords(text):
    stop_words = set(stopwords.words("english"))
    words = nltk.word_tokenize(text)
    words = [word.lower() for word in words if word.isalpha()]
    words = [word for word in words if word not in stop_words]
    word_ngrams = ngrams(words, 2)
    word_ngrams = [' '.join(ngram) for ngram in word_ngrams]
    words += word_ngrams
    word_freq = nltk.FreqDist(words)
    keywords = word_freq.most_common(10)
    keywords = [keyword for keyword, freq in keywords]
    return keywords

def cluster_name(text):
    keywords = extract_keywords(text)
    return keywords

def Tag_prediction(filename):
  with open(filename,'r') as f:
    text = f.read()
  new_text_corpus =  dictionary.doc2bow(text.split())
  prob_topics = lda_model.get_document_topics(new_text_corpus)
  prob_topics.sort(key = lambda x: x[1],reverse=True)
  print(prob_topics)
  output_tags = []
  cluster_sugg = cluster_name(text)
  print(cluster_sugg)
  terms = text.split()
  output_tags = list(Tags_new[prob_topics[0][0]].keys())[:4]
  return output_tags

Tag_prediction("/content/drive/MyDrive/BTP_TestSet_Mails/test1.txt")

Tag_prediction("/content/drive/MyDrive/BTP_TestSet_Mails/test2.txt")

Tag_prediction("/content/drive/MyDrive/BTP_TestSet_Mails/test13.txt")

"""# Evaluation"""

def model_prediction(text):
  new_text_corpus =  dictionary.doc2bow(text.split())
  prob_topics = lda_model.get_document_topics(new_text_corpus)
  prob_topics.sort(key = lambda x: x[1],reverse=True)
  output_tags = []
  terms = text.split()
  output_tags = list(Tags_new[prob_topics[0][0]].keys())[:4]
  print(output_tags)
  return output_tags

test = []
for i in range(1, 101):
  filename = "/content/drive/MyDrive/BTP_TestSet_Mails/test" + str(i) + ".txt"
  with open(filename, "r") as f:
    text = f.read().strip().lower().replace('\n', '')
    test.append(text)

import pandas as pd

test_data_path = "/content/drive/MyDrive/BTP_Aishu/GoldSet_BTP - Sheet1.csv"
data = pd.read_csv(test_data_path)
data['Tags'] = data['Tags'].str.replace(" ", "")

data

class MailTester:
  def __init__(self, test_mails, test_data, model):
    self.test_mails = test_mails
    self.model = model
    self.test_data = test_data
    self.test()
  
  def test(self):
    correct_predictions = 0
    total_predictions = 0

    for index, row in self.test_data.iterrows():
        mail_body = self.test_mails[index]
        expected_tags = set(row['Tags'].split(','))
        predicted_tags = set(model_prediction(mail_body))#set(self.model.predict(mail_body))
        common_tags = set(expected_tags).intersection(predicted_tags)
        if len(common_tags) > 0:
          correct_predictions += 1
        total_predictions += 1

    accuracy = correct_predictions / total_predictions
    print(f"Accuracy: {accuracy}")

tester = MailTester(test, data, None)

